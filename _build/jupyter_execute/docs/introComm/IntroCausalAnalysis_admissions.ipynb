{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tbonne/peds/blob/main/docs/introComm/IntroCausalAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9C2arduFECBl"
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/tbonne/peds/main/images/COLLEGE-ACCEPTANCE.jpg\" width=\"500\" alt=\"colab\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQpnBRMDe_mB"
   },
   "source": [
    "## <font color='darkorange'>Explainability vs Causality</font>\n",
    "\n",
    "\n",
    "Here we will look at the difference between understanding how the ML model is making predictions (explainability) and what is causing the outcome (causality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFmN1aLjfVej"
   },
   "source": [
    "To do so we will look at a university admission example. You have been hired and asked to decided whether there is a gender bias in admission, and if there is reason for legal action against the university.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4-8qJBvfiL1"
   },
   "source": [
    "### <font color='darkorange'>Gender and university admissions</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6oL7k7uftq4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZ7MxOMHflXV"
   },
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NHlsf8GeMWW"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "df_admit = pd.read_csv(\"/content/UCB_admission.csv\")\n",
    "\n",
    "#take a look\n",
    "df_admit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing data, and the types of data we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FovWxCFQkiTn"
   },
   "source": [
    "Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some exploratory data analysis before build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lm36a2uZmIW5"
   },
   "outputs": [],
   "source": [
    "#plot admissions by reported gender\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot admissions by department\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What patterns do you see?\n",
    "\n",
    "Do these visualizations help you answer if there is systematic bias in acceptance rates by gender?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMJVKC1ckysB"
   },
   "source": [
    "### <font color='darkorange'>Preprocessing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some categorical predictor variables so let's do some preprocessing!\n",
    "\n",
    "Let's one-hot-encode 'dept'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ByGVB4fLmIG"
   },
   "outputs": [],
   "source": [
    "#convert the categorical variable into dummy variables\n",
    "df_cat = pd.get_dummies(df_admit['dept'])\n",
    "\n",
    "#concat the dummy variables back onto the dataframe\n",
    "df_admit = pd.concat([df_admit, df_cat], axis = 1)\n",
    "\n",
    "#drop the original categorical variable\n",
    "df_admit = df_admit.drop(['dept'], axis=1)\n",
    "\n",
    "#take a look\n",
    "df_admit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode the binary predictor variable gender as 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUnoEdV7AZLP"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#build the encoder\n",
    "my_gen = LabelEncoder()\n",
    "\n",
    "#fit and transform the gender column\n",
    "df_admit['applicant.gender'] = my_gen.fit_transform(df_admit['applicant.gender'].values.reshape(-1,1))\n",
    "\n",
    "#take a look\n",
    "df_admit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKdW7X6B0epK"
   },
   "source": [
    "Finally, let's do a training testing split on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uuaVICoRlMs"
   },
   "outputs": [],
   "source": [
    "#split data into predictors (X) and target (y)\n",
    "X = df_admit.drop('admitted', axis=1)\n",
    "y = df_admit['admitted']\n",
    "\n",
    "#split these data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAk0s-hKmH7i"
   },
   "source": [
    "### <font color='darkorange'>Build a model</font>\n",
    "\n",
    "Can we predict admission based on reported gender?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqXwuYoe8Va9"
   },
   "source": [
    "Build a random forest for predicting admission using gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHrG5Z1lRJ5q"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#1. Build the model\n",
    "forest_classifier = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=0.8, max_samples=0.8, max_depth=5,random_state=243)\n",
    "\n",
    "#2. Fit the model to the data\n",
    "forest_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what variables the model learnt were important for predicting if someone will be admitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "#model interpretation\n",
    "rel_impo = permutation_importance(forest_classifier, X_test, y_test,n_repeats=30,random_state=243)\n",
    "\n",
    "#build a dataframe to store the results\n",
    "df_rel_impo = pd.DataFrame({\"feature\":X_test.columns,\"importance\":rel_impo.importances_mean, \"sd\":rel_impo.importances_std})\n",
    "\n",
    "#take a look\n",
    "df_rel_impo.sort_values(by='importance', ascending=False,inplace=True)\n",
    "\n",
    "df_rel_impo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the permutation feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df_rel_impo, y='feature', x='importance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's ask our model to do some counterfactual. These are \"what-if\" questions that we can use to see what would have happened if an applicant would be admitted base on if they reported a different gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a dataframe for the conterfactual you want to test\n",
    "df_question = pd.DataFrame({\n",
    "    'applicant.gender': [0, 1],\n",
    "    'school_score': [50, 50],\n",
    "    'A': [0, 0],\n",
    "    'B': [0, 0],\n",
    "    'C': [1, 1],\n",
    "    'D': [0, 0],\n",
    "    'E': [0, 0],\n",
    "    'F': [0, 0],\n",
    "})\n",
    "\n",
    "# 2. Use the model to get probabilities\n",
    "question_pred_proba = forest_classifier.predict_proba(df_question)\n",
    "\n",
    "# 3. Wrap into a DataFrame for readability\n",
    "proba_df = pd.DataFrame(\n",
    "    question_pred_proba,\n",
    "    columns=forest_classifier.classes_,\n",
    "    index=['Female', 'Male']\n",
    ")\n",
    "\n",
    "print(proba_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the model telling us when it comes to the impact that gender has on if someone will be admitted to UC Berkeley?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l9Slvyc72dT"
   },
   "source": [
    "### <font color='darkorange'>Fit the model again, this time with a reduced model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To highlight how our model changes interpretation base on what variables we include, let's fit the model again... this time without accounting for department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a smaller model - remove departments\n",
    "X_train_small = X_train.drop([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"], axis=1)\n",
    "\n",
    "X_test_small = X_test.drop([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a new random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mweD6mtJSYPh"
   },
   "outputs": [],
   "source": [
    "#1. Build the model\n",
    "forest_classifier_small = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features=0.8, max_samples=0.8, max_depth=5,random_state=243)\n",
    "\n",
    "#2. Fit the model to the data\n",
    "forest_classifier_small.fit(X_train_small, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate permutation feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model interpretation\n",
    "rel_impo_small = permutation_importance(forest_classifier_small, X_test_small, y_test,n_repeats=30,random_state=243)\n",
    "\n",
    "#build a dataframe to store the results\n",
    "df_rel_impo_small = pd.DataFrame({\"feature\":X_test_small.columns,\"importance\":rel_impo_small.importances_mean, \"sd\":rel_impo_small.importances_std})\n",
    "\n",
    "#take a look\n",
    "df_rel_impo_small.sort_values(by='importance', ascending=False,inplace=True)\n",
    "\n",
    "df_rel_impo_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df_rel_impo_small, y='feature', x='importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's ask the model a counterfactual question about how gender might impact admissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. dataframe for the scenarios you want to test\n",
    "df_question = pd.DataFrame({\n",
    "    'applicant.gender': [0, 1],\n",
    "    'school_score': [0, 0]\n",
    "})\n",
    "\n",
    "# 2. get probabilities\n",
    "question_pred_proba = forest_classifier_small.predict_proba(df_question)\n",
    "\n",
    "# 3. take a look\n",
    "proba_df = pd.DataFrame(\n",
    "    question_pred_proba,\n",
    "    columns=forest_classifier.classes_,\n",
    "    index=['Female', 'Male']\n",
    ")\n",
    "\n",
    "print(proba_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that now the model predicts quite a bit of difference. Which one is right? \n",
    "\n",
    "The answer is both! \n",
    "\n",
    "It's just that each model is telling us something different. By containing different predictor variables, the models are addressing a different question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkorange'>Statistical confounds</font>\n",
    "\n",
    "> Statistical confounds make it hard to determine the causal nature of the patterns we find in ML model results. This is the case with traditional statistical models as well! We need to be careful about how we explain how a model makes predictions and the causal nature of those patterns.\n",
    "\n",
    "> In the case of the admissions and gender, there is a process where genders are not applying to departments in equal measure.\n",
    "\n",
    "That is, the causal relationships that generated this data might look something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/tbonne/peds/main/images/DAG_admission.png\" width=\"600\" alt=\"colab\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with all the predictors in, is estimating the bottom line going from Gender to Admission. That is called the \"direct effect\" of Gender.\n",
    "\n",
    "While the model without Department is estimating the lines going from Gender to Department then to Admission, and the line going from Gender to admission. This is called the \"total effect\" of Gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/tbonne/peds/main/images/take_action.jpg\" width=\"100\" alt=\"colab\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try going back and changing/removing the fixed random state, do you get the same answer each time.\n",
    "\n",
    "Also, school score is just a random number! I added it in as a way for you to double check if your model is overfitting. See if you can build random forest models that correctly give zero weight to school score, and yet still pull out the impacts of gender and department. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkorange'>Feature selection by performance</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we use recursive feature extraction to automatically choose parameters for us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "#min number of variables/features\n",
    "min_features_to_select = 1\n",
    "\n",
    "#build the feature selection algorithm\n",
    "rfecv = RFECV(estimator=forest_classifier, step=1, cv=5, min_features_to_select=min_features_to_select)\n",
    "\n",
    "#fit the algorithm to the data\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Number of features tested\n",
    "n_features = range(\n",
    "    rfecv.min_features_to_select,\n",
    "    len(rfecv.cv_results_[\"mean_test_score\"]) + rfecv.min_features_to_select\n",
    ")\n",
    "\n",
    "# Plot the scores\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(n_features, rfecv.cv_results_[\"mean_test_score\"], marker=\"o\")\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"CV score (mean across folds)\")\n",
    "plt.title(\"Recursive Feature Elimination with Cross-Validation (RFECV)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selected_features = X_train.columns[rfecv.support_]\n",
    "print(\"Selected features:\", list(selected_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did the feature selector suggest is the \"best\" features to include? \n",
    "* Does this model help you answer the question about if one gender is more likely to be admitted to UC Berkeley?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NK_IenAS48i1"
   },
   "source": [
    "#### <font color='darkorange'>Bonus</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lEDh3SO4_4u"
   },
   "source": [
    "<p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/tbonne/peds/main/images/take_action.jpg\" width=\"100\" alt=\"colab\">\n",
    "</p>\n",
    "\n",
    "Redo the exercise above this time using decision trees, or linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# build \n",
    "LR1 = LinearRegression()\n",
    "\n",
    "# fit\n",
    "LR1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgxVHlWrjkry"
   },
   "source": [
    "### <font color='darkorange'>Further reading</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZmp46mF7reu"
   },
   "source": [
    "If you would like to know more about causal inference you might like:\n",
    "\n",
    "> The book of why ([link](http://repo.darmajaya.ac.id/4847/1/The%20book%20of%20why_%20the%20new%20science%20of%20cause%20and%20effect%20%28%20PDFDrive%20%29.pdf))\n",
    "\n",
    "> Causal Inference and Discovery in Python ([link](https://github.com/PacktPublishing/Causal-Inference-and-Discovery-in-Python))\n",
    "\n",
    "> If you would like the notebook without missing code check out the [full code](https://colab.research.google.com/github/tbonne/peds/blob/main/docs/fullNotebooks/full_IntroCausalAnalysis_admissions.ipynb) version."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN3HQqgWw6dGvAp7C4PrcfC",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1ZSejLEaSqtPw_BnKWwNvlKloiaTPp-zf",
   "name": "IntroCausalAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}