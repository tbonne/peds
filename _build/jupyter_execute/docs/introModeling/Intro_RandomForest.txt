import sklearn as sk
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split


#get wine to a dataframe
df_diab = pd.read_csv('')

#take a look
?


?

?

#count how many of each value in a column using value_conunts
df_diab.?.value_counts()

?

?


sns.scatterplot(data=df_diab, x='?', y='?')

sns.violinplot(data=df_diab, y='?', x='?')


?


from sklearn.ensemble import RandomForestClassifier

#1. Build the model
forest_classifier = RandomForestClassifier(n_estimators=1000, bootstrap=True, max_features=0.8,max_samples=0.8)

#2. Fit the model to the data
forest_classifier.fit(X_train, y_train)

from sklearn.tree import DecisionTreeClassifier

#1. Build the model
tree_classifier = ?()

#2. Fit the model to the data
?.fit(?, ?)

#predictions from the forest model
y_forest_pred = forest_classifier.predict(X_test)

#predictions from the tree model
y_tree_pred = tree_classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
?


#more visual approach
?

print('Accuracy (forest): {:.2f}'.format(sk.metrics.accuracy_score(y_test, y_forest_pred)))
print('Accuracy (tree): {:.2f}'.format(sk.metrics.accuracy_score(y_test, y_tree_pred)))
print('Null Accuracy: {:.2f}'.format(1-(y_train.sum()/(y_train.count()))))


print('Precision (tree): {:.2f}'.format(sk.metrics.precision_score(y_test, y_tree_pred)))
print('Recall (tree): {:.2f}'.format(sk.metrics.recall_score(y_test, y_tree_pred)))

from sklearn.model_selection import cross_val_score

number_of_trees = [50, 100, 150, 200, 250, 300, 350]

for val in number_of_trees:
  scores = cross_val_score(RandomForestClassifier(n_estimators=val), X_train, y_train, cv=5, scoring='accuracy')
  print(scores.mean())

from sklearn.model_selection import GridSearchCV

#define what parameters and what values to vary
parameters = {'max_features': [0.5,0.7,0.9,1.0],
              'n_estimators':list(range(50,200,50)),
              'max_samples':[0.5,0.7,0.9,0.99] }

#build the grid search algorithm
grid_search = GridSearchCV(RandomForestClassifier(), parameters, cv=5, scoring='accuracy') #strattified cross validation when traget is binary or multiclass

#Use training data to perform the nfold cross validation
grid_search.fit(X_train, y_train)

#find the best hyperparameters
print(grid_search.best_params_)
grid_search.best_score_

#1. build an optimized model
forest_classifier_opt = RandomForestClassifier(n_estimators=?,max_samples=?, max_features=?)

#2. Fit the model to the data
?.fit(?, ?)

#3. make predictions
y_forest_pred_opt = ?.predict(?)

#Measure accuracy
print('Accuracy: {:.2f}'.format(sk.metrics.accuracy_score(y_test, y_forest_pred_opt)))
print('Precision : {:.2f}'.format(sk.metrics.precision_score(y_test, y_forest_pred_opt)))
print('Recall : {:.2f}'.format(sk.metrics.recall_score(y_test, y_forest_pred_opt)))

#calculate a confusion matrix
?

#Plot the confusion matrix
?

#Try adding other hyperparameters, how much better can you make the model by tuning?
#e.g., min_samples_split (how many points in a node are required to allow a split)
#e.g., max_depth (max depth of each tree)

from sklearn.inspection import permutation_importance

#use permutation importance
perm_result = permutation_importance(forest_classifier_opt, X=X_test, y=y_test, scoring='accuracy', n_repeats=30)

#place values into a dataframe
forest_importances = pd.DataFrame({'variable':X_test.columns,'impo':perm_result.importances_mean.round(4), "sd":perm_result.importances_std.round(4)})

#sort the dataframe
forest_importances.sort_values(by='impo', ascending=False)

#plot the importance
sns.barplot(data=forest_importances, x='variable',y='impo')
plt.xticks(rotation=70)

#plot the importance (switch axis to display labels better?)
sns.barplot(data=forest_importances, y='variable',x='impo')


#1. Create a dataframe
df_question = pd.DataFrame({'Pregnancies':X_train.Pregnancies.mean(),
                            'Glucose':X_train.Glucose.mean(),
                            'BloodPressure':X_train.BloodPressure.mean(),
                            'SkinThickness':X_train.SkinThickness.mean(),
                            'Insulin':X_train.Insulin.mean(),
                            'BMI':X_train.BMI.mean(),
                            'DiabetesPedigreeFunction':X_train.DiabetesPedigreeFunction.mean(),
                            'Age':X_train.Age.mean()},
                             index=[0])
                            

#2. Use the model to make predictions
question_pred =  forest_classifier_opt.predict(df_question)

#3. Take a look at the answer
question_pred

#1. Create a dataframe
df_question = pd.DataFrame({'Pregnancies':X_train.Pregnancies.mean(),
                            'Glucose':list(range(0,200,10)),
                            'BloodPressure':X_train.BloodPressure.mean(),
                            'SkinThickness':X_train.SkinThickness.mean(),
                            'Insulin':X_train.Insulin.mean(),
                            'BMI':X_train.BMI.mean(),
                            'DiabetesPedigreeFunction':X_train.DiabetesPedigreeFunction.mean(),
                            'Age':X_train.Age.mean()})
                            

#2. Use the model to make predictions
question_pred =  forest_classifier_opt.predict(df_question)

#3. Take a look at the answer
question_pred

#add a column to the df_question
df_question['predicted_diabetes'] = question_pred

#plot the predictions
sns.scatterplot(data=df_question, x='Glucose',y='predicted_diabetes')

#load dataset!
