import pandas as pd
import sklearn as sk
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split

#load data
df_waffles = pd.read_csv("/content/waffles.csv")

#take a look
df_waffles.head()

#sort the dataframe
pd_df = df_waffles.sort_values(['Divorce']).reset_index(drop=True)

#plot by state
sns.barplot(data=pd_df, x="Loc",y="Divorce")
plt.xticks(rotation=90)

#correlation
df_waffles.WaffleHouses.corr(df_waffles.Divorce)

#scatter plot
sns.scatterplot(data=df_waffles, x="WaffleHouses", y="Divorce" )


#split these data into training and testing datasets
df_train, df_test = train_test_split(df_waffles, test_size=0.20, random_state=14)

import statsmodels.api as sm #for running regression!
import statsmodels.formula.api as smf

#Build the model
linear_reg_model = smf.ols(formula='Divorce ~ WaffleHouses ', data=df_train)

#Use the data to fit the model (i.e., find the best intercept and slope parameters)
linear_reg_results = linear_reg_model.fit()

#summary
print(linear_reg_results.summary())

#Build the model
linear_reg_model_South = smf.ols(formula='Divorce ~ WaffleHouses + South', data=df_train)

#Use the data to fit the model (i.e., find the best intercept and slope parameters)
linear_reg_model_South = linear_reg_model_South.fit()

#summary
print(linear_reg_model_South.summary())

sns.boxplot(data=df_waffles, x="South", y="WaffleHouses")

from sklearn.linear_model import LinearRegression
from sklearn.inspection import permutation_importance

#split data into predictors (X) and target (y)
X = df_waffles.drop(['Divorce','Location','Loc'],axis=1)
y = df_waffles['Divorce']

#split these data into training and testing datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

#fit linear regression
LR1 = LinearRegression()
LR1.fit(X_train, y_train)

#model interpretation
rel_impo = permutation_importance(LR1, X_test, y_test,n_repeats=30,random_state=0)
pd.DataFrame({"feature":X_test.columns,"importance":rel_impo.importances_mean, "sd":rel_impo.importances_std})

from sklearn.model_selection import KFold
from sklearn.feature_selection import RFECV

#split data into predictors (X) and target (y)
X = df_waffles.drop(['Divorce','Location','Loc'], axis=1)
y = df_waffles['Divorce']

#split these data into training and testing datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

#build a linear regression (full model)
LR1 = LinearRegression()

#fit linear regression
LR1.fit(X_train, y_train)

#min number of variables/features
min_features_to_select = 1

#build the feature selection algorithm
rfecv = RFECV(estimator=LR1, step=1, cv=3,scoring='neg_mean_squared_error', min_features_to_select=min_features_to_select)

#fit the algorithm to the data
rfecv.fit(X_train, y_train)

print("Optimal number of features : %d" % rfecv.n_features_)

# Plot number of features VS. cross-validation scores
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Cross validation score (mean square error?)")
plt.plot(range(min_features_to_select,
               len(rfecv.grid_scores_) + min_features_to_select),
         rfecv.grid_scores_)
plt.show()

rfecv.support_

X_train_reduced = X_train.iloc[:,rfecv.support_]

X_train_reduced.head(3)

#get the slopes!
rfecv.estimator_.coef_

from sklearn.model_selection import train_test_split

#split data into predictors (X) and target (y)
X = df_waffles[['Population','Marriage','WaffleHouses']]
y = df_waffles['Divorce']

#split these data into training and testing datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

parameters = {'max_depth': list(range(1, 10,2)),
              'n_estimators':list(range(50,200,50)),
              'max_samples':[0.5,0.7,0.9,0.99]}

randForest = RandomForestRegressor()
grid_search = GridSearchCV(randForest, parameters, cv=5, n_jobs=1, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

sorted(grid_search.cv_results_.keys())
print(grid_search.best_params_)




#1. build the model
RFR = RandomForestRegressor(max_depth=5,n_estimators=50,max_samples=0.5)

#2. fit the model to the data
RFR.fit(X_train, y_train)

#3. make predictions using the model
y_pred = RFR.predict(X_test)

#how well does it predict
from sklearn.metrics import mean_squared_error

mean_squared_error(y_test,y_pred)

#What is important for prediction?
from sklearn.inspection import permutation_importance

#estimate permutation importance on the test data
perm_impo = permutation_importance(RFR, X_test, y_test,scoring='neg_mean_squared_error',n_repeats=30)

#create a dataframe with the values
df_imp = pd.DataFrame({"feature":X_test.columns,"importance":perm_impo.importances_mean, "sd":perm_impo.importances_std})
sorted(sk.metrics.SCORERS.keys())

#take a look
df_imp

#plot the importance values
df_imp_all = pd.DataFrame(perm_impo.importances.transpose())
df_imp_all.columns = X_test.columns
df_imp_all_long = pd.melt(df_imp_all)
sns.barplot(data=df_imp_all_long, x="variable",y="value", ci=95)
plt.xticks(rotation=90) 


#1. Create a dataframe
df_question = pd.DataFrame({'Population':X_train.Population.mean(),
                            'WaffleHouses':list(range(0,200,10)),
                            'Marriage':X_train.Marriage.mean(),
                            })
                            

#2. Use the model to make predictions
question_pred =  RFR.predict(df_question)

#3. add a column to the df_question
df_question['predicted_divorceRates'] = question_pred

#4. plot the predictions
sns.scatterplot(data=df_question, x='WaffleHouses',y='predicted_divorceRates')
question_pred

#split data into predictors (X) and target (y)
X = df_waffles[['Population','Marriage','WaffleHouses','South']]
y = df_waffles['Divorce']


#split these data into training and testing datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

RFR2 = RandomForestRegressor(max_depth=5,n_estimators=50,max_samples=0.5)
RFR2.fit(X_train, y_train)

mean_squared_error(RFR2.predict(X_test), y_test)

rel_impo2 = permutation_importance(RFR2, X_test, y_test,n_repeats=30,random_state=0)

pd.DataFrame({"feature":X_test.columns,"importance":rel_impo2.importances_mean, "sd":rel_impo2.importances_std})

df_imp_all = pd.DataFrame(rel_impo2.importances.transpose())
df_imp_all.columns = X_test.columns
df_imp_all_long = pd.melt(df_imp_all)
sns.barplot(data=df_imp_all_long, x="variable",y="value", ci=95)
plt.xticks(rotation=90)


#1. Create a dataframe
df_question = pd.DataFrame({'Population':X_train.Population.mean(),
                            'WaffleHouses':list(range(0,200,10)),
                            'Marriage':X_train.Marriage.mean(),
                            'South':1
                            })
                            

#2. Use the model to make predictions
question_pred =  RFR2.predict(df_question)

#3. add a column to the df_question
df_question['predicted_divorceRates'] = question_pred

#4. plot the predictions
sns.scatterplot(data=df_question, x='WaffleHouses',y='predicted_divorceRates')
question_pred
