{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbonne/peds/blob/main/docs/introModeling/IntroModeling3_decisionTrees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C2arduFECBl"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/tbonne/peds/main/images/tree_decision.jpg\" width=\"500\" alt=\"colab\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDo_JqBMQqJM"
      },
      "source": [
        "## <font color='darkorange'>Decision trees</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HwGNjO9Qt6G"
      },
      "source": [
        "Let's take a look at decision tree as classifiers. Here we will use this algorithm to classify which grape plant was used to create a wine. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0x5is0FE_j9"
      },
      "source": [
        "Load in the needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-rPIThpIE3o"
      },
      "outputs": [],
      "source": [
        "import sklearn as sk\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z7obQJoGmr5"
      },
      "source": [
        "### <font color='darkorange'>Load the data</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwYGOotUQfqo"
      },
      "source": [
        "Get the 'wine.csv' from the class's shared data folder and load it into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCpMYU5fQiM-"
      },
      "outputs": [],
      "source": [
        "#get wine to a dataframe\n",
        "df_wine = pd.read_csv('/content/wine_labs.csv')\n",
        "\n",
        "#take a look\n",
        "df_wine.head(3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTUwaLV9HdvV"
      },
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1WC4tXGCEF-1_2LQ74gIxJAZ-GLXCwBdK' width=\"100\" align = 'left'>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN6DonAlHYuX"
      },
      "source": [
        "Q: what kinds of data are we dealing with?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-4mDkjCHbsc"
      },
      "outputs": [],
      "source": [
        "df_wine.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlYmLGWeTpri"
      },
      "source": [
        "Q: are there any missing values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN9UdxsATt8D"
      },
      "outputs": [],
      "source": [
        "df_wine.isna().sum() / len(df_wine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cd-Xg-1svxe"
      },
      "outputs": [],
      "source": [
        "#let's drop rows with missing data\n",
        "df_wine = df_wine.dropna(how='any')\n",
        "\n",
        "#take a look\n",
        "df_wine.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfcRG5tmRhOS"
      },
      "source": [
        "### <font color='darkorange'>Descriptive statistics</font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kptMv845FjpI"
      },
      "source": [
        "Let's take a little time to look at some summary statistics.\n",
        " \n",
        "E.g., how many values of plant type there are? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG4Am986FjTM"
      },
      "outputs": [],
      "source": [
        "#count how many of each value in a column using value_conunts\n",
        "df_wine.plant.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLe7t6uCGiSb"
      },
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1WC4tXGCEF-1_2LQ74gIxJAZ-GLXCwBdK' width=\"100\" align = 'left'>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-_xBgiiFyAO"
      },
      "source": [
        "Choose one feature (column) and get the mean, min, and max."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIYqTxojGAqj"
      },
      "outputs": [],
      "source": [
        "df_wine.total_sugar.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsIoTD89RnSH"
      },
      "source": [
        "### <font color='darkorange'>Visualizing the data</font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi5f6tbMHLjl"
      },
      "source": [
        "Let's plot the relationships between plant type and some of the wine measures.\n",
        " \n",
        "Q: Choose one or more wine measures and generate a plot that shows the relationship between that measure and plant type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP1hbowmH4zH"
      },
      "outputs": [],
      "source": [
        "sns.violinplot(data=df_wine, x='plant', y='total_sugar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozz-07c9RwXe"
      },
      "source": [
        "### <font color='darkorange'>Data wrangling</font> \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before building our models we have to do some preprocessing steps with our data. We'll cover some common steps here, but we'll see more along the way.\n",
        "\n",
        "Let's first consider categorical predictor variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Preprocessing (categorical input variables)**\n",
        "> Convert the categorical 'lab' variable using onehot encoding (i.e., create many dummy columns to replace the one categorical column).\n",
        "\n",
        "> Note: we could also convert the categories to numbers (Ordinal) but this assumes that there is an order to the categories which isn't always a good assumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty-cDLrJEil-"
      },
      "outputs": [],
      "source": [
        "#categorical variables\n",
        "cat_names = ['lab']\n",
        "\n",
        "#create dummy variables\n",
        "df_cat = pd.get_dummies(df_wine[cat_names])\n",
        "\n",
        "#add them back to the original dataframe\n",
        "df_wine = pd.concat([df_wine,df_cat], axis=1)\n",
        "\n",
        "#remove the old columns\n",
        "df_wine = df_wine.drop(cat_names, axis=1)\n",
        "\n",
        "#take a look\n",
        "df_wine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-a14jmIPse1"
      },
      "source": [
        "Next let's look at how we might convert a categorical outcome variable.\n",
        "\n",
        "**Preprocessing (categorical outcome variable)**\n",
        "> As the outcome variable is categorical we will convert each category into a number, and unlike the onehot encoding we will keep these numbers within the same column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGA16i5mXUZd"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#create the encoder\n",
        "le_plants = LabelEncoder()\n",
        "\n",
        "#create outcome variable\n",
        "df_wine['plant'] = le_plants.fit_transform(df_wine['plant'])\n",
        "\n",
        "#take a look\n",
        "df_wine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then always get back what plant category corresponds to which numeric label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "le_plants.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wngk9w1YMLY"
      },
      "source": [
        "#### Training testing split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPPwnrSFKsk-"
      },
      "source": [
        "We will follow a general approach when building models. We will divide the dataset into *training* and *testing* datasets.\n",
        "This lets us fit the model to one part of the data and then use the withheld data to test the predictions of the model. This helps us detect and avoid *overfitting* our model!\n",
        "\n",
        "To do this we'll first split the dataframe into inputs and target variables. I.e., we'd like to use X to predict y. Then we split each into training and testing sets. This makes it easier to work with sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcziOvoDYO6I"
      },
      "outputs": [],
      "source": [
        "\n",
        "#split data into predictors (X) and target (y)\n",
        "X = df_wine.drop('plant', axis=1)\n",
        "y = df_wine['plant']\n",
        "\n",
        "#split these data into training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've split our data, we can do some more processing. Here let's scale our numeric variables. Importantly, we'll build our scaling function from the training data and apply it to the testing. That way we are only using information from the training data and not contaminating our model with testing data information as well. I.e., no data leakage between training and testing datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQFF6vqbH2hX"
      },
      "source": [
        "**Preprocessing (numeric variables)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VkAXoajH6DV"
      },
      "outputs": [],
      "source": [
        "#Feature Scaling (after spliting the data!)\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "\n",
        "#numeric variables\n",
        "numb_names = X_train.drop(['lab_lab1','lab_lab2','lab_lab3'],axis=1).select_dtypes('number').columns.tolist()\n",
        "\n",
        "#create the standard scaler object\n",
        "sc = StandardScaler()\n",
        "\n",
        "#use this object to fit (i.e., to calculate the mean and sd of each variable in the training data) and then to transform the training data\n",
        "X_train[numb_names] = sc.fit_transform(X_train[numb_names])\n",
        "\n",
        "#use the fit from the training data to transform the test data\n",
        "X_test[numb_names] = sc.transform(X_test[numb_names])\n",
        "\n",
        "#take a look\n",
        "X_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0MmbEZ7dq5e"
      },
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1qWrKY9TgpgQaBCzZfz1xLTV6iCeSwfmG' width=\"100\" align = 'left'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2flRXo0eR_ur"
      },
      "source": [
        "### <font color='darkorange'>Model building</font> \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cekIelOqSG2B"
      },
      "source": [
        "Here we will build our first decision tree!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWRmQuhdSH5h"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#1. build the algorithm\n",
        "classifier = DecisionTreeClassifier()\n",
        "\n",
        "#2. fit the algorithm to the data\n",
        "classifier_res= classifier.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-gbWapVa6N_"
      },
      "source": [
        "**Predictions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1Mwjlurbeux"
      },
      "source": [
        "Make some predictions on the testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIKo-fbPbBG7"
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEr16lBebd6X"
      },
      "source": [
        "Measure classification success"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew9BnKJDblBY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVJQdODwcclq"
      },
      "outputs": [],
      "source": [
        "#more visual approach\n",
        "sns.heatmap(cm, annot=True)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXdo-AEHcnQf"
      },
      "source": [
        "More detailed metrics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXbkeYN3cqgN"
      },
      "outputs": [],
      "source": [
        "print(f'Accuracy: {sk.metrics.accuracy_score(y_test, y_pred):.2f}')\n",
        "print(f'Precision: {sk.metrics.precision_score(y_test, y_pred, average='micro'):.2f}')\n",
        "print(f'Recal: {sk.metrics.recall_score(y_test, y_pred, average='micro'):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aocKt6vRdLXv"
      },
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1qWrKY9TgpgQaBCzZfz1xLTV6iCeSwfmG' width=\"100\" align = 'left'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCt43JabdIFP"
      },
      "source": [
        "**Hyperparameters**\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0VQBpYGdn-z"
      },
      "source": [
        "Decision tree algorithms have a number of hyperparameters that can be tuned to acheive better predictions. Let's take a look at one and how we can tune it!\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8sd8Sjxf3G-"
      },
      "source": [
        "First we need some way to test how model performance varies as we change the parameter. We can't use the testing dataset... if we did it really wouldn't be a good test. I.e., the testing data would be used to help build the model and so would not be independent. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29gVnbDugQiY"
      },
      "source": [
        "So let's split the training dataset again! This will create training and validation datasets! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-0Z9yLPfMfY"
      },
      "outputs": [],
      "source": [
        "X_hyper_train, X_hyper_val, y_hyper_train, y_hyper_val = train_test_split(X_train, y_train, test_size=0.20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNefL3ZNgXXu"
      },
      "source": [
        "Let's next focus on the max depth parameter, and see if we can find a value that maximizes the performance of the model on the validation dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6xL8D6zf1Vc"
      },
      "source": [
        "\n",
        "> We'll first build a function that takes as input max depth, and outputs the accuracy score.\n",
        " \n",
        "> We'll then use a loop to try out many max depth scores.\n",
        " \n",
        "> Finally we'll plot the the accuracy scores for each max depth value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPk77gYteS0x"
      },
      "source": [
        "Let's first build a function the takes max depth as input and outputs a accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj5-cqamdkrL"
      },
      "outputs": [],
      "source": [
        "def fit_decision_tree(maxDep):\n",
        "\n",
        "  #1. build the algorithm\n",
        "  classifier = DecisionTreeClassifier(max_depth=maxDep)\n",
        "\n",
        "  #2. Fit the algorithm\n",
        "  classifier_res= classifier.fit(X_hyper_train, y_hyper_train)\n",
        "\n",
        "  #3. Make predictions\n",
        "  y_pred = classifier.predict(X_hyper_val)\n",
        "\n",
        "  #4. Meausure the accuracy\n",
        "  accuracy_measured = sk.metrics.accuracy_score(y_hyper_val, y_pred)\n",
        "\n",
        "  return accuracy_measured"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaViV2XIhBJF"
      },
      "source": [
        "Try out your new function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6vG0eNqgxfO"
      },
      "outputs": [],
      "source": [
        "fit_decision_tree(maxDep=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsTMPU2_hD7D"
      },
      "source": [
        "Next let's build a loop and see what values of max depth give the best results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW8cukwLhDi4"
      },
      "outputs": [],
      "source": [
        "acc_scores = []\n",
        "for i in range(1,10):\n",
        "  acc_s = fit_decision_tree(i)\n",
        "  acc_scores.append(acc_s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv-Vs2EWhX6z"
      },
      "source": [
        "Then let's plot it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0F17L0ShXod"
      },
      "outputs": [],
      "source": [
        "#create a dataframe\n",
        "df_plot_maxDep = pd.DataFrame({'accuracy':acc_scores, 'maxDep':range(1,10)})\n",
        "\n",
        "#make a plot\n",
        "sns.relplot(data=df_plot_maxDep, x='maxDep',y='accuracy', kind='line')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J78EywXFZJdd"
      },
      "source": [
        "### <font color='darkorange'>Bonus</font> \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9DFqK7uXocJ"
      },
      "source": [
        "Try this exercise again but this time use min_samples_split. Which is the parameter that defines when splits are no longer considered (e.g., if a leaf has 10 points in it and min_samples_split is 11 then the algorithm will not look to split the leaf.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI0IEnJlihQb"
      },
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1qWrKY9TgpgQaBCzZfz1xLTV6iCeSwfmG' width=\"100\" align = 'left'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nsw2I5da-tH"
      },
      "source": [
        "### <font color='darkorange'>Model interpretation</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73ZYQcMIfmiN"
      },
      "outputs": [],
      "source": [
        "!pip install dtreeviz\n",
        "import dtreeviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision trees can offer some nice visuals that can help interpret and communicate your results, lets take a look at a few.\n",
        "\n",
        "First let's see how what splits the model found in the training data, and how many data points fell within each leaf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c6XmZvIfVAo"
      },
      "outputs": [],
      "source": [
        "#build the figure\n",
        "viz = dtreeviz.model(classifier_res, X_train, y_train,\n",
        "                target_name=\"plant\",\n",
        "                feature_names=X_train.columns.to_list(),\n",
        "                class_names={0:'plantA',1:'plantB',2:'plantC'}\n",
        "                )\n",
        "\n",
        "#take a look\n",
        "viz.view(fontname=\"DejaVu Sans\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1_3LH25bBdZ"
      },
      "source": [
        "Now let's look at how the tree does with the test data.\n",
        "\n",
        "In this case, we can see how the test samples fall into the fixed structure of the tree. Do you see any big differences in leaf sizes and mixtures?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_KikDvjbAuA"
      },
      "outputs": [],
      "source": [
        "#build the figure\n",
        "viz_test = dtreeviz.model(classifier_res, X_test, y_test,\n",
        "                target_name=\"plant\",\n",
        "                feature_names=X_train.columns.to_list(),\n",
        "                class_names={0:'plantA',1:'plantB',2:'plantC'}\n",
        "                )\n",
        "\n",
        "#take a look\n",
        "viz_test.view(fontname=\"DejaVu Sans\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's see how one data point in the test data gets place within the tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qO4lQfTa3yO"
      },
      "outputs": [],
      "source": [
        "#Vizualize one prediction\n",
        "import numpy as np\n",
        "\n",
        "# pick random X test point\n",
        "X_values_for_pred = X_test.iloc[12] #you can choose any other row!\n",
        "\n",
        "X_values_for_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0XdYC31cU7s"
      },
      "outputs": [],
      "source": [
        "viz_obj = viz_test.view(\n",
        "    x=X_values_for_pred,\n",
        "    fontname=\"DejaVu Sans\"\n",
        ")\n",
        "\n",
        "viz_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiraBYwQ5Awh"
      },
      "source": [
        "### <font color='darkorange'>Bonus 2</font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP_Pgh9n9Pcw"
      },
      "source": [
        "Now that you know how to use decision tree models, try going back to one of the datasets we've already worked on and see if you can get better predictions? Can you still explain what features are helping you to make those predictions?\n",
        "\n",
        "Note: decision trees work well for both categorical outcome variables (as shown above) and continuous/integer outcome variables. The difference is in how the splits are made:\n",
        "* categorical models split based on class impurity e.g., gini impurity or entropy.\n",
        "* regressor models split based on variation e.g., mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-YZRflb9jAC"
      },
      "outputs": [],
      "source": [
        "#load a dataset"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNhbeIvBJUWIvy+CHjAXwvZ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "IntroModeling3_decisionTrees.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
